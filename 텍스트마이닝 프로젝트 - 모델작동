#엑셀파일 불러오기
import pandas as pd
data = pd.read_excel('서울 해고 불용어 처리 문장이랑 라벨.xlsx')
data = data.drop(['Unnamed: 0'], axis = 1)

#필요한 라이브러리 불러오기
!pip install konlpy
from konlpy.tag import Okt # komoran, hannanum, kkma, mecab
import os
import numpy as np
import pandas as pd
from datetime import datetime
import json
import re
from tqdm.notebook import tqdm

#훈련, 테스트셋 분리
from sklearn.model_selection import train_test_split
x, y = data.내용, data.라벨
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)

#벡터화 라이브러리 호출
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

okt = Okt()
#okt.nouns를 tokenizer로 이용
tfidf = TfidfVectorizer(tokenizer=okt.nouns, max_features=2000, min_df=20, max_df=0.5) 

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

#로지스틱 회귀
clf = LogisticRegression(C=1.0, max_iter=1000) # 모델 결정
clf.fit(X_train_tfidf, y_train) # 모델 학습
print('#Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data score
print('#Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data score

# 로지스틱 회귀모델은 L2규제(C=1.0이 기본)를 기본으로 하며, 원한다면 penalty = l1으로 바꾸어 L1 규제를 사용가능 
# C값이 작아졌을 때, 즉 규제가 강화되었을 때는, 가중치 변화가 거의 0에 수렴, 비용(cost)값이 작아졌다고 보시면 됨.
# 반면 C의 값이 커져서 규제가 약화된다면, 학습률에 따라서 가중치의 변화가 커짐.

# 과적합 해결방법 
# 데이터의 수를 늘린다.
# 모델의 Complexity를 줄인다, 파라미터 수가 적은 모델을 선택하거나, 모델에 제약을 가하여 단순화
# Regularization을 사용한다
# 훈련 데이터의 잡음을 줄이기(Outlier, Error 제거)

#실제값과 예측값 비교
for content in zip(y_test[:30], clf.predict(X_test_tfidf[:30]), X_test[:30]):
    print(content)
    

from sklearn.naive_bayes import MultinomialNB

NB_clf = MultinomialNB(alpha=1.0)
NB_clf.fit(X_train_tfidf, y_train)
print('Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train)))
print('Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test)))




from sklearn.ensemble import RandomForestClassifier

#스코어에 주로 영향을 끼치는 요소는 뎁스
model_rf = RandomForestClassifier(n_estimators = 100, max_depth=30, random_state = 0)
model_rf.fit(X_train_tfidf, y_train)
print('Train set score: {:.3f}'.format(model_rf.score(X_train_tfidf, y_train)))
print('Test set score: {:.3f}'.format(model_rf.score(X_test_tfidf, y_test)))



from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

model_ada = AdaBoostClassifier(n_estimators=100, base_estimator=DecisionTreeClassifier(max_depth=5),
                               algorithm='SAMME.R', learning_rate=0.5)
model_ada.fit(X_train_tfidf, y_train)
print('Train set score: {:.3f}'.format(model_ada.score(X_train_tfidf, y_train)))
print('Test set score: {:.3f}'.format(model_ada.score(X_test_tfidf, y_test)))


import xgboost as xgb

model_xgb = xgb.XGBClassifier(booster = 'gbtree',
                             learning_rate = 0.5,
                              n_estimators= 100,
                             max_depth=5)

model_xgb.fit(X_train_tfidf, y_train)
print('Train set score: {:.3f}'.format(model_xgb.score(X_train_tfidf, y_train)))
print('Test set score: {:.3f}'.format(model_xgb.score(X_test_tfidf, y_test)))

